{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd as ag\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_layers= nn.Sequential(nn.Conv2d(1, 32, (3,3), padding=1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2)),\n",
    "                                 nn.Conv2d(32, 64, (3,3), padding=1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2)))\n",
    "        self.linear_layers = nn.Sequential(\n",
    "                                 nn.Linear(3136, 100),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(100, 50),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(50, 2),\n",
    "                                 nn.Softmax()\n",
    "                                 # nn.MaxPool2d((2,2))\n",
    "                                 )\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        conv_output = self.conv_layers(inp)\n",
    "        return self.linear_layers(conv_output.view(conv_output.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 128, (5, 5)), # 14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, (5, 5)), # 18\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, (5, 5)), # 22\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, (3, 3)), # 24\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, (3, 3)), # 26\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, (3, 3)), # 28\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inp = Variable(torch.rand( 1, 1, 28, 28))\n",
    "d = Discriminator()\n",
    "num_epochs = 1\n",
    "train_loader = data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor()])))\n",
    "test_loader = data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor()])))\n",
    "\n",
    "count =0\n",
    "for batch, (train_data, target) in enumerate(train_loader):\n",
    "    if count == 1000:\n",
    "        break\n",
    "    count +=1\n",
    "    # print(\" count : \", count)\n",
    "    # print(\"train : \", train_data)\n",
    "    # print(\"target : \", target)\n",
    "    for epoch in range(num_epochs):\n",
    "        output = d.forward(Variable(train_data))\n",
    "        optimizer = optim.Adam(d.parameters())\n",
    "        # target = Variable(target)\n",
    "        # target_t = torch.zeros(1, 10)\n",
    "        # print(target_t[0][target-1])\n",
    "        # target_t[0][target] = 1\n",
    "        target_t = torch.Tensor([[1, 0]])\n",
    "        # print(\"size \", target.size())\n",
    "        loss_obj = nn.MSELoss()\n",
    "        loss = loss_obj(output, Variable(target_t))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\" epoch : \", epoch, \"loss : \", loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  0\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  1\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  2\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  3\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  4\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  5\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  6\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  7\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch :  8\n tot loss :  0.6265233755111694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-80e09060a135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shravan/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shravan/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "g = Generator()\n",
    "z = torch.rand(1,1,10,10)\n",
    "gen_epochs = 100\n",
    "# print(g.forward(Variable(z)))\n",
    "# print(len(train_loader[0]))\n",
    "count = 0 \n",
    "for batch, (train_data, target) in enumerate(test_loader):\n",
    "    if count >= 100:\n",
    "        break\n",
    "    count +=1\n",
    "    for epoch in range(gen_epochs):\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        d_loss = ce_loss(d.forward(Variable(train_data)), Variable(torch.LongTensor(1)))\n",
    "        z = Variable(torch.rand(1,1,10,10))\n",
    "        g_loss = ce_loss(d.forward(g.forward(z)), Variable(torch.LongTensor(1)))\n",
    "        tot_loss = d_loss + g_loss\n",
    "        optimizer_d = optim.SGD(d.parameters(), lr=1e-3)\n",
    "        tot_loss.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        optimizer_g = optim.SGD(g.parameters(), lr=1e-3)\n",
    "        g_loss = ce_loss(d.forward(g.forward(z)), Variable(torch.LongTensor(1)))\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        if epoch == 99:\n",
    "            print(\" batch : \", batch)\n",
    "            print(\" tot loss : \", tot_loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}